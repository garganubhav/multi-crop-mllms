import torchvision.transforms.functional as TF
import numpy as np
import os
from scipy.ndimage import median_filter
from skimage.measure import block_reduce


def high_pass_filter(image, resolusion, km=7, kh=3, reduce=True):
    """
    Applies a high-pass filter to an image to highlight edges and fine details.
    
    This function resizes the image, applies a Gaussian blur to create a low-frequency version,
    subtracts it from the original to get high-frequency components, and then applies median filtering.
    
    Args:
        image: Input PIL image
        resolusion: Target resolution to resize the image to
        km: Kernel size for median filtering (default: 7)
        kh: Kernel size for Gaussian blur (default: 3)
        reduce: Whether to reduce the output size using block reduction (default: True)
        
    Returns:
        h_brightness: A 2D numpy array representing the high-frequency components of the image
    """

    image = TF.resize(image, (resolusion, resolusion))
    image = TF.to_tensor(image).unsqueeze(0)
    l = TF.gaussian_blur(image, kernel_size=(kh, kh)).squeeze().detach().cpu().numpy()
    h = image.squeeze().detach().cpu().numpy() - l
    h_brightness = np.sqrt(np.square(h).sum(axis=0))
    h_brightness = median_filter(h_brightness, size=km)
    if reduce:
        h_brightness = block_reduce(h_brightness, block_size=(14, 14), func=np.sum)

    return h_brightness

# def bbox_from_att_image_adaptive(att_map, image_size, bbox_size=336):
#     """
#     Generates an adaptive bounding box for original image from an attention map.
    
#     This function finds the region with the highest attention in the attention map
#     and creates a bounding box around it. It tries different crop ratios and selects
#     the one that produces the sharpest attention difference.
    
#     Args:
#         att_map: A 2D numpy array representing the attention map (e.g., 24x24 for LLaVA or 16x16 for BLIP)
#         image_size: Tuple of (width, height) of the original image
#         bbox_size: Base size for the bounding box (default: 336)
        
#     Returns:
#         tuple: (x1, y1, x2, y2) coordinates of the bounding box in the original image
#     """

#     # the ratios corresponds to the bounding box we are going to crop the image
#     ratios = [1, 1.2, 1.4, 1.6, 1.8, 2]

#     max_att_poses = []
#     differences = []
#     block_nums = []

#     for ratio in ratios:
#         # perform a bbox_size*r width and bbox_size*r height crop, where bbox_size is the size of the model's original image input resolution. (336 for LLaVA, 224 for BLIP)

#         # the size of each block in the attention map, in the original image
#         block_size = image_size[0] / att_map.shape[1], image_size[1] / att_map.shape[0]

#         # if I want a bbox_size*r width and bbox_size*r height crop from the original image, the number of blocks I need (x, y)
#         block_num = min(int(bbox_size*ratio/block_size[0]), att_map.shape[1]), min(int(bbox_size*ratio/block_size[1]), att_map.shape[0])
#         if att_map.shape[1]-block_num[0] < 1 and att_map.shape[0]-block_num[1] < 1:
#             if ratio == 1:
#                 return 0, 0, image_size[0], image_size[1]
#             else:
#                 continue
#         block_nums.append((block_num[0], block_num[1]))
        
#         # attention aggregation map
#         sliding_att = np.zeros((att_map.shape[0]-block_num[1]+1, att_map.shape[1]-block_num[0]+1))
#         max_att = -np.inf
#         max_att_pos = (0, 0)

#         # sliding window to find the block with the highest attention
#         for x in range(att_map.shape[1]-block_num[0]+1): 
#             for y in range(att_map.shape[0]-block_num[1]+1): 
#                 att = att_map[y:y+block_num[1], x:x+block_num[0]].sum()
#                 sliding_att[y, x] = att
#                 if att > max_att:
#                     max_att = att
#                     max_att_pos = (x, y)
        
#         # we have the position of max attention, we can calculate the difference between the max attention and the average of its adjacent attentions, to see if it is sharp enough, the more difference, the sharper
#         # we choose the best ratio r according to their attention difference
#         adjcent_atts = []
#         if max_att_pos[0] > 0:
#             adjcent_atts.append(sliding_att[max_att_pos[1], max_att_pos[0]-1])
#         if max_att_pos[0] < sliding_att.shape[1]-1:
#             adjcent_atts.append(sliding_att[max_att_pos[1], max_att_pos[0]+1])
#         if max_att_pos[1] > 0:
#             adjcent_atts.append(sliding_att[max_att_pos[1]-1, max_att_pos[0]])
#         if max_att_pos[1] < sliding_att.shape[0]-1:
#             adjcent_atts.append(sliding_att[max_att_pos[1]+1, max_att_pos[0]])
#         difference = (max_att - np.mean(adjcent_atts)) / (block_num[0] * block_num[1])
#         differences.append(difference)
#         max_att_poses.append(max_att_pos)
#     max_att_pos = max_att_poses[np.argmax(differences)]
#     block_num = block_nums[np.argmax(differences)]
#     selected_bbox_size = bbox_size * ratios[np.argmax(differences)]
    
#     x_center = int(max_att_pos[0] * block_size[0] + block_size[0] * block_num[0] / 2)
#     y_center = int(max_att_pos[1] * block_size[1] + block_size[1] * block_num[1] / 2)
    
#     x_center = selected_bbox_size//2 if x_center < selected_bbox_size//2 else x_center
#     y_center = selected_bbox_size//2 if y_center < selected_bbox_size//2 else y_center
#     x_center = image_size[0] - selected_bbox_size//2 if x_center > image_size[0] - selected_bbox_size//2 else x_center
#     y_center = image_size[1] - selected_bbox_size//2 if y_center > image_size[1] - selected_bbox_size//2 else y_center

#     x1 = max(0, x_center - selected_bbox_size//2)
#     y1 = max(0, y_center - selected_bbox_size//2)
#     x2 = min(image_size[0], x_center + selected_bbox_size//2)
#     y2 = min(image_size[1], y_center + selected_bbox_size//2)

#     return x1, y1, x2, y2

def bbox_from_att_image_adaptive(att_map, image_size, bbox_size=336, threshold_fraction=0.8):
    """
    Generates adaptive bounding boxes for original image from an attention map, returning all boxes
    with attention above a fraction of the maximum raw attention.
    
    Args:
        att_map: A 2D numpy array representing the attention map (e.g., 24x24 for LLaVA or 16x16 for BLIP)
        image_size: Tuple of (width, height) of the original image
        bbox_size: Base size for the bounding box (default: 336)
        threshold_fraction: Fraction of maximum raw attention to use as threshold (default: 0.8)
        
    Returns:
        list: List of (x1, y1, x2, y2, attention) tuples for all bounding boxes above the threshold
    """

    ratios = [1, 1.2, 1.4, 1.6, 1.8, 2]
    block_size = image_size[0] / att_map.shape[1], image_size[1] / att_map.shape[0]
    all_bboxes = []

    for ratio in ratios:
        block_num = (min(int(bbox_size * ratio / block_size[0]), att_map.shape[1]),
                     min(int(bbox_size * ratio / block_size[1]), att_map.shape[0]))
        
        if att_map.shape[1] - block_num[0] < 1 and att_map.shape[0] - block_num[1] < 1:
            if ratio == 1:
                return [(0, 0, image_size[0], image_size[1], att_map.sum())]
            else:
                continue
        
        sliding_att = np.zeros((att_map.shape[0] - block_num[1] + 1, att_map.shape[1] - block_num[0] + 1))
        att_positions = []  # Store (x, y, attention) for all positions
        max_att = -np.inf  # Track maximum raw attention for this ratio

        # Sliding window to collect all attention values and find the maximum
        for x in range(att_map.shape[1] - block_num[0] + 1):
            for y in range(att_map.shape[0] - block_num[1] + 1):
                att = att_map[y:y + block_num[1], x:x + block_num[0]].sum()
                sliding_att[y, x] = att
                att_positions.append((x, y, att))
                if att > max_att:
                    max_att = att

        # Set threshold as a fraction of the maximum raw attention
        threshold = max_att * threshold_fraction

        # Filter positions based on threshold
        selected_positions = [(x, y, att) for x, y, att in att_positions if att >= threshold]

        # Convert each selected position to bounding box coordinates
        selected_bbox_size = bbox_size * ratio
        for pos_x, pos_y, att in selected_positions:
            # Calculate center in pixel coordinates
            x_center = int(pos_x * block_size[0] + block_size[0] * block_num[0] / 2)
            y_center = int(pos_y * block_size[1] + block_size[1] * block_num[1] / 2)

            # Adjust center to fit within image boundaries
            x_center = selected_bbox_size // 2 if x_center < selected_bbox_size // 2 else x_center
            y_center = selected_bbox_size // 2 if y_center < selected_bbox_size // 2 else y_center
            x_center = image_size[0] - selected_bbox_size // 2 if x_center > image_size[0] - selected_bbox_size // 2 else x_center
            y_center = image_size[1] - selected_bbox_size // 2 if y_center > image_size[1] - selected_bbox_size // 2 else y_center

            # Compute bounding box corners
            x1 = max(0, x_center - selected_bbox_size // 2)
            y1 = max(0, y_center - selected_bbox_size // 2)
            x2 = min(image_size[0], x_center + selected_bbox_size // 2)
            y2 = min(image_size[1], y_center + selected_bbox_size // 2)

            # Store the bounding box with its raw attention value
            all_bboxes.append((x1, y1, x2, y2, att))

    return all_bboxes

def high_res_split_threshold(image, res_threshold=1024):
    """
    Splits a high-resolution image into smaller patches.
    
    This function divides a large image into smaller patches to process them individually,
    which is useful for handling high-resolution images that might be too large for direct processing.
    
    Args:
        image: Input PIL image
        res_threshold: Maximum resolution threshold before splitting (default: 1024)
        
    Returns:
        tuple: (split_images, vertical_split, horizontal_split)
            - split_images: List of PIL image patches
            - vertical_split: Number of vertical splits
            - horizontal_split: Number of horizontal splits
    """

    vertical_split = int(np.ceil(image.size[1] / res_threshold))
    horizontal_split = int(vertical_split * image.size[0] / image.size[1])

    split_num = (horizontal_split, vertical_split)
    split_size = int(np.ceil(image.size[0] / split_num[0])), int(np.ceil(image.size[1] / split_num[1]))
    
    split_images = []
    for j in range(split_num[1]):
        for i in range(split_num[0]):
            split_image = image.crop((i*split_size[0], j*split_size[1], (i+1)*split_size[0], (j+1)*split_size[1]))
            split_images.append(split_image)
    
    return split_images, vertical_split, horizontal_split

def high_res(map_func, image, prompt, general_prompt, model, processor):
    """
    Applies an attention mapping function to high-resolution images by splitting and recombining.
    
    This function splits a high-resolution image into smaller patches, applies the specified
    attention mapping function to each patch, and then recombines the results into a single
    attention map.
    
    Args:
        map_func: The attention mapping function to apply to each patch
        image: Input PIL image
        prompt: Text prompt for the attention function
        general_prompt: General text prompt for baseline comparison
        model: Model instance (LLaVA or BLIP)
        processor: Processor for the corresponding model
        
    Returns:
        block_att: A 2D numpy array representing the combined attention map for the entire image
    """

    split_images, num_vertical_split, num_horizontal_split = high_res_split_threshold(image)
    att_maps = []
    for split_image in split_images:
        att_map = map_func(split_image, prompt, general_prompt, model, processor)
        # att_map = att_map / att_map.mean()
        att_maps.append(att_map)
    block_att = np.block([att_maps[j:j+num_horizontal_split] for j in range(0, num_horizontal_split * num_vertical_split, num_horizontal_split)])

    return block_att
